---
layout:     post                    # 使用的布局（不需要改）
title:      爬虫學習               # 标题 
subtitle:   爬虫实战1                #副标题
date:       2018-10-09            # 时间
author:     Euraxluo                      # 作者
header-img: img/post-bg-github-cup.jpg  #这篇文章标题背景图片
catalog: true                 # 是否归档
tags:                               #标签
    - 爬虫

---
## 分布式爬虫

### https://beautifulsoup.readthedocs.io/zh_CN/latest/
### http://docs.python-requests.org/zh_CN/latest/user/quickstart.html
### xpath http://www.cheat-sheets.org/saved-copy/Locators_groups_1_0_2.pdf
### css http://www.cheat-sheets.org/saved-copy/Locators_table_1_0_2.pdf

最后只打算爬109万数据，没有用到分布式，最终还是不会，讲一讲爬虫实战的注意事项

### 1.Hearders：
不需要Cookie{不需要模拟登陆}，Renfer,只需要UA就好了，找多个UA，每轮随机选择一个作为header就好了
### 2.proxies：
需要多轮筛选才能保证下载时的效率，不然会卡很久
### 3.设置下载阻塞结构，没有下载成功无法跳过：
```python
for i in rang(10):
	while true:
		try:
			a()
			break
		except:
			pass
```
由于下载代码块是阻塞的，必须写单页下载器，不能写的很乱
### 4.lxml selector：
不使用`>`可以跳级，缩短解析路径
### 5.没有实现，或者使用的功能
scrapy框架，下一次使用
分布式，数据量不大，没有虚拟机/服务器
多线程，多进程：

## 6.PYTHON 的线程{thread 库}，一个是 进程{multiprocessing库}
1-基本概念
```
并发：指的是任务数多余cpu核数，通过操作系统的各种任务调度算法，

        实现⽤多个任务“⼀起”执⾏（实际上总有⼀些任务不在执⾏，因为切换任
        务的速度相当快，看上去⼀起执⾏⽽已）
并⾏：指的是任务数⼩于等于cpu核数，即任务真的是⼀起执⾏的

多线程（threading）：
①在⼀个进程内的所有线程共享全局变量，很⽅便在多个线程间共享数据
②缺点就是，线程是对全局变量随意遂改可能造成多线程之间对全局变量的混乱（即线程⾮安全）
③如果多个线程同时对同⼀个全局变量操作，会出现资源竞争问题，从⽽数据结果会不正确
```

2-python中鸡肋的线程：

```
1、GIL是什么？

GIL的全称是Global Interpreter Lock(全局解释器锁)，来源是python设计之初的考虑，为了数据安全所做的决定。

2、每个CPU在同一时间只能执行一个线程（在单核CPU下的多线程其实都只是并发，不是并行，并发和并行从宏观上来讲都是同时处理多路请求的概念。但并发和并行又有区别，并行是指两个或者多个事件在同一时刻发生；而并发是指两个或多个事件在同一时间间隔内发生。）

在Python多线程下，每个线程的执行方式：

获取GIL
执行代码直到sleep或者是python虚拟机将其挂起。
释放GIL
可见，某个线程想要执行，必须先拿到GIL，我们可以把GIL看作是“通行证”，并且在一个python进程中，GIL只有一个。拿不到通行证的线程，就不允许进入CPU执行。

在Python2.x里，GIL的释放逻辑是当前线程遇见IO操作或者ticks计数达到100（ticks可以看作是Python自身的一个计数器，专门做用于GIL，每次释放后归零，这个计数可以通过 sys.setcheckinterval 来调整），进行释放。

而每次释放GIL锁，线程进行锁竞争、切换线程，会消耗资源。并且由于GIL锁存在，python里一个进程永远只能同时执行一个线程(拿到GIL的线程才能执行)，这就是为什么在多核CPU上，python的多线程效率并不高。
```
3-MongoDB对并发的支持
```
1. MongoDB锁的类型
　在2.2版本中MongoDB用的是读写锁，允许并行的读但是只能互斥的写，当一个读锁存在的时候可以有多个读操作共享这个锁，但是当一个写锁存在的时候只能有一个写操作获得这个锁，其它的读或者写不能共享这个锁。
　在2.2版本中写锁是"贪婪"的，意味着写比读有更大的优先权，当一个读和写操作正在等待一个锁的时候，MongoDB会优先满足写操作的锁要求。
　在3.0版本中MongoDB的锁机制就有了比较大的改进，跟常见的数据库锁机制比较相似了。3.0还是使用读写锁机制，只是支持了多粒度的锁，支持全局、数据库、集合这几个粒度的锁（锁的粒度待到下面章节来详细了解）。MongoDB支持插件式的存储引擎，这样允许存储引擎自己来实现比集合粒度更细的并发控制(例如在3.0版本中引入了WiredTiger引擎，这个引擎支持文档粒度的锁)。
　在3.0中除了"共享锁S"(我个人理解就是读锁)和"互斥锁X"(也就是写锁)以外还加入了"意向共享锁IS"和"意向互斥锁IX"，这两种类型的锁预示我们后面需要更细粒度的锁来读写资源。当我们用某一粒度的锁以后所有比这个粒度大的地方都用"意向锁"来锁定。
　例如，当锁定一个集合用于写（使用X锁）的时候，相应的数据库锁和全局锁必须用意向锁（IX锁）来锁定。一个数据库能够同时被IS、IX模式锁定，但是一个互斥锁X不能和其它的锁模式并存，一个共享锁S只能和意向共享锁IS同时存在。
　在3.0版本中锁是公平的（不像2.2版本中写锁有更大的优先权），读和写顺序在队列中排队。然而，为了优化吞吐量，当一个请求被授权，其它兼容的请求都会在同一时刻被授权，这样就可能在遇到矛盾的请求之前就已经释放了这个请求。例如，刚刚一个X锁被释放，冲突队列中包含了如下的锁：

IS → IS → X → X → S → IS

　如果按照严格的先进先出（FIFO），只有最开始的两个IS请求会被释放，但是MongoDB会把所有跟IS兼容的IS、S请求都同时释放。一旦这些请求被释放，MongoDB接着就会释放X，即使有新的IS、S请求到来，也就是说MongoDB只会释放队列中最前面的请求，这样就不会有请求被"饿死"。

2. 锁的粒度
　从2.2版本开始，MongoDB实现了数据库级别的锁，对于大部分的读写操作都用数据库锁即可，但是一些全局操作，通常涉及到多个数据库操作的时候还是需要全局锁。在2.2版本之前MongoDB只有全局锁，例如，如果有6个数据库那么其中一个数据库的写操作不会影响其它5个数据库的读写操作的，但是这在2.2之前是不行的。
　在MongoDB 3.0版本中锁的粒度就变得更细了，除了全局锁、数据库锁还加入了集合锁，而且对于WiredTiger存储引擎和MMAPv1存储引擎而言两者之间的锁机制也有不同。
　WiredTiger：对于大部分的读写操作，WiredTiger使用乐观锁。WiredTiger对于全局、数据库、集合级别只会使用意向锁。当存储引擎检测到两个操作之间的冲突，一个写冲突导致MongoDB透明地重试写操作。一些全局操作，跟2.2版本一样还是会需要全局锁，例如，删除一个集合，那么仍然还是需要一个互斥的数据库锁的。
　MMAPv1：3.0版本MMAPv1引擎用集合锁，相比之前的版本数据库锁是最细粒度的锁而言有了更进一步的改进。例如，在使用MMAPv1作为存储引擎的数据库中有6个集合，当其中一个集合写锁存在的时候，其它5个集合仍然可以自由的使用读锁、写锁来进行读写操作。

3. 如何查看当前MongoDB锁的状态
　MongoDB提供了如下的命令来查看当前的锁状态：

db.serverStatus(),
db.currentOp(),
mongotop,
mongostat, and/or
the MongoDB Management Service (MMS)
　楼主一般用currentOp来查看当前MongoDB的锁的状态，具体的可以参考文档。
　如果遇到一个慢查询导致锁一直没释放的可以参考我这篇文章MongoDB一次性能问题处理。

4. 读写操作是否会主动让出锁？
　在某些情况下，读写操作会主动让出自己拥有的锁。
　长时间的读或者写操作，例如query、update、delete，都会在很多情况下主动让出锁。
　MongoDB的MMAPv1引擎使用启发模式来预测要读取的数据是否在内存中，如果预测数据没有在内存中那么在把数据从硬盘加载到内存的过程中这个读锁就会主动让出，一旦数据加载完，这个读操作就会重新获得锁。
　对于支持文档级别并发控制的存储引擎，如WiredTiger，当存在全局、数据库、集合级别的意向锁的时候没有必要主动让出锁，因为这些锁并不会完全阻塞读、写操作。

5. 常见操作对应的锁
　MongoDB中的一些常见的操作对应的锁可以参考如下的两个链接：
　- Which operations lock the database?
　- Which administrative commands lock the database?
　- Does a MongoDB operation ever lock more than one database?

6. sharding、replica set对并发的影响
　在sharding模式下每一个mongod实例都是独立于分片集群中其它实例的包括它的锁，一个mongod实例中的锁不会影响其它实例。
　在replica set模式下因为要保持primary、secondaries之间的同步，所以当在primary写入数据的时候MongoDB同步更新primary中的oplog（oplog是一个特殊的集合在local数据库中），因此MongoDB会同时锁住两个数据库以保证同步。

7. MongoDB不支持事务

8. 结论：所有操作都修改成mongodb的update operators ($set,$push,$inc)了。mongodb默认是有锁的，也就是说只要使用了mongodb的更新操作器的原子操作之后，就不会有脏数据写入了
```

4-协程

```python
1.协程:可以视作一种用户级的轻量级线程。协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方,在切回来的时候，恢复先前保存的寄存器上下文和栈。因此,协程能保留上一次调用时的状态。每次过程重入时，就相当于进入上一次调用的状态，换种说法：进入上一次离开时所处逻辑流的位置。在并发编程中，协程与线程类似，每个协程表示一个执行单元，有自己的本地数据，与其它协程共享全局数据和其它资源
```
结论：最后的并发策略是进程加协程

### 注意事项：
1.异常处理，在爬虫过程中应当全部提交异常给最上层的控制程序处理，在请求失败次数不到阈值时，对任务队列进行修复
2.超市请求：请求超时的时间很难掌控，一方面，超时时间过长会让程序运行时间过长，如果超时时间过短，又会让请求时频繁抛出异常，这也说明了，代理池的稳定性多么重要
3.函数的死锁和超时：需要自己处理，因为python是顺序执行的，如果发生死锁，有没有超时设置，又不·{import signal}处理，会让代码莫名停止，对于自动化爬虫，这是毁灭性的
4.自重启：如果程序自己没有大问题，又多次报错，让其自动重启！
5.多协程爬虫会占用带宽，也就是说，受带宽限制，不要分配过多的协程！
6.当有重复数据需要删除时使用MongoDB的update来更新数据，而非用insert插入，具体如下：
```
db.collection.update( query, update, { upsert: , multi: , writeConcern: })
参数说明： 
query : update的查询条件，类似sql update查询内where后面的。 
update : update的对象和一些更新的操作符（如,inc…）等，也可以理解为sql update查询内set后面的 
upsert : 可选，这个参数的意思是，如果不存在update的记录，是否插入objNew,true为插入，默认是false，不插入。 
multi : 可选，mongodb 默认是false,只更新找到的第一条记录，如果这个参数为true,就把按条件查出来多条记录全部更新。 
writeConcern :可选，抛出异常的级别。
```