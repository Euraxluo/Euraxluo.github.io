---
layout:     post                    # 使用的布局（不需要改）
title:      爬虫學習               # 标题 
subtitle:   爬虫学习1                #副标题
date:       2018-10-28            # 时间
author:     Euraxluo                      # 作者
header-img: img/post-bg-github-cup.jpg  #这篇文章标题背景图片
catalog: true                 # 是否归档
tags:                               #标签
    - 爬虫

---
## 前记：
+ 爬虫：使用任何技术手段，批量获取网站信息的一种方式。关键在于批量。
+ 反爬虫：使用任何技术手段，阻止别人批量获取自己网站信息的一种方式。关键也在于批量。
+ 误伤：在反爬虫的过程中，错误的将普通用户识别为爬虫。误伤率高的反爬虫策略，效果再好也不能用。
+ 拦截：成功地阻止爬虫访问。这里会有拦截率的概念。通常来说，拦截率越高的反爬虫策略，误伤的可能性就越高。因此需要做个权衡。
+ 资源：机器成本与人力成本的总和。

## url 管理器：管理待抓取url集合和已抓取url集合

个人：set(),python的set()可以自动去重
大量带爬取url：关系数据库mysql
互联网公司：缓存数据库(高性能)

## 网页下载器：
### 1.urllib2：python官方基础模块（py2.7）
下载方法：
1.直接下载 
```python
import urllib2
response = urllib2.urlopen(url)#直接下载
print response.getcode()#获取状态码
cont = response.read()#读取内容
```
2.伪装和密码
```python 
import urllib2
request = urllib2.Request(url)#创建request对象
request.add_data('a','l')#添加数据，a-l,诸如账号密码
request.add_header('User-Agent','Mozilla/5.0')#添加http的header，用于伪装
response = urllib2.urlopen(request)#发送请求获取结果
cont = response.read()#读取内容
```
3.复杂情景（加套子）
HTTPCookie用户登录情景/Proxy代理信息/HTTPS加密信息/Readirect防止URL互相跳转
```python
import urllib2,cookielib
cj = cookielib.CookieJar()#创建cookie容器
opener = urllib2.builb_opener(urllib2.HTTPCookieProcessor(cj))#httpcookie用户登陆
urllib2.intall_opener(opener)#给urllib2安装opener
response = urllib2.urlopen(url)#使用带有cookie的urllib2爬取网页
```

### 2.urllib.request:(py3)
#### 2.1 request.urlopen方法： 
##### `urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None) `
+ urlopen无法判断数据的encoding，所以返回的是bytes对象。一般会对返回的数据进行decode。
+ url参数，可以是一个string，或者一个Request对象。
+ data一定是bytes对象，传递给服务器的数据，或者为None。目前只有HTTP requests会使用data，提供data时会是一个post请求，如若没有data，那就是get请求。data在使用前需要使用urllib.parse.urlencode()函数转换成流数据
 ##### urlopen方法：
-  read() , readline() ,readlines() , fileno() , close() ：对HTTPResponse类型数据进行操作 
-  info()：返回HTTPMessage对象，表示远程服务器返回的头信息 
-  getcode()：返回Http状态码。如果是http请求，200请求成功完成;404网址未找到 
-  geturl()：返回请求的url
##### eg:
```python
from urllib import request
req = request.urlopen('http://euraxluo.cn')
print(req.read().decode())#read()方法是读取返回数据内容，decode是转换返回数据的bytes格式为str 
```
#### 2.2 urllib.request.Reques：

`urllib.request.Request(url, data=None, headers={},origin_req_host=None, unverifiable=False, method=None)`
##### eg:
```python
import urllib.request
req=urllib.request.Request('http://euraxluo.cn')
with urllib.request.urlopen(req) as response:
    page=response.read(300).decode('utf-8')#我们获取的数据一般是ascii的，decode成utf-8.
print(page)
```

#### 2.3 用来包装头部的数据： 
+ User-Agent ：这个头部可以携带如下几条信息：浏览器名和版本号、操作系统名和版本号、默认语言 
```常用爬虫UA
百度图片
“Baiduspider-image+(+http://www.baidu.com/search/spider.htm)”

百度最新UA如下：
PC：
Mozilla/5.0 (compatible; Baiduspider-render/2.0; +http://www.baidu.com/search/spider.html)
移动：
Mozilla/5.0 (iPhone; CPU iPhone OS 9_1 like Mac OS X) AppleWebKit/601.1.46 (KHTML, like Gecko) Version/9.0 Mobile/13B143 Safari/601.1 (compatible; Baiduspider-render/2.0; +http://www.baidu.com/search/spider.html)

360搜索
Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0);

360网站安全检测
360spider (http://webscan.360.cn)

Google
“Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)”

Google图片搜索
“Googlebot-Image/1.0”

Adwords移动网络
“AdsBot-Google-Mobile (+http://www.google.com/mobile/adsbot.html) Mozilla (iPhone; U; CPU iPhone OS 3 0 like Mac OS X) AppleWebKit (KHTML, like Gecko) Mobile Safari”

微软 bing，必应
“Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)”

腾讯搜搜
“Sosospider+(+http://help.soso.com/webspider.htm)”

搜搜图片
“Sosoimagespider+(+http://help.soso.com/soso-image-spider.htm)”

雅虎英文
“Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)”

雅虎中国
“Mozilla/5.0 (compatible; Yahoo! Slurp China; http://misc.yahoo.com.cn/help.html)”

搜狗图片
“http://pic.sogou.com” “Sogou Pic Spider/3.0(+http://www.sogou.com/docs/help/webmasters.htm#07)”

搜狗
“Sogou web spider/4.0(+http://www.sogou.com/docs/help/webmasters.htm#07)”

网易有道
“Mozilla/5.0 (compatible; YoudaoBot/1.0; http://www.youdao.com/help/webmaster/spider/; )”

瑞典 Speedy Spider
“Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) Speedy Spider (http://www.entireweb.com/about/search_tech/speedy_spider/)”

俄罗斯 yandex
“Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)”

宜搜 EasouSpider
Mozilla/5.0 (compatible; EasouSpider; +http://www.easou.com/search/spider.html)

华为赛门铁克蜘蛛
“HuaweiSymantecSpider/1.0+DSE-support@huaweisymantec.com+(compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR ; http://www.huaweisymantec.com/cn/IRL/spider)”
华为赛门铁克科技有限公司网页信誉分析系统的一个页面爬取程序，其作用是用于爬取互联网网页并进行信誉分析，从而检查该网站上的是否含有恶意代码。

七牛镜像蜘蛛
qiniu-imgstg-spider-1.0

监控宝
“Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; JianKongBao Monitor 1.1)”

DNSPod监控
DNSPod-Monitor/2.0
```
+ Referer：可以用来防止盗链，有一些网站图片显示来源http://*.com，就是检查Referer来鉴定的 
+ Connection：表示连接状态，记录Session的状态
```
#urllib.request.Request(url, data=None, headers={}, method=None)
#使用request（）来包装请求，再通过urlopen（）获取页面。
```


#### 2.4 Post数据

+ urlopen()的data参数默认为None，当data参数不为空的时候，urlopen（）提交方式为Post
`urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)`
`
urlencode（）主要作用就是将url附上要提交的数据。

+ Post的数据必须是bytes或者iterable of bytes，不能是str，因此需要进行encode（）编码
page = request.urlopen(req, data=data).read()
当然，也可以把data的数据封装在urlopen（）参数中
`data = {'first': 'true',}`
`data = parse.urlencode(data).encode('utf-8')`

+ eg:
```python
import urllib.parse as up
import urllib.request as ur
url = 'http://www.uustv.com/'
values = {'word': 'Euraxluo','sizes': '60','fonts': 'jfcs.ttf','fontcolor': '#000000'}#Post的数据
data = up.urlencode(values)#编码
data = data.encode('ascii')#解码,server,只接受ascii数据
req = ur.Request(url,data)
with ur.urlopen(req) as response:
    page = response.read().decode('utf-8')  #我们获取的数据一般是ascii的，decode成utf-8.
print(page)
```
#### 使用代理
+ 当需要抓取的网站设置了访问限制，这时就需要用到代理来抓取数据
`urllib.request.ProxyHandler(proxies=None)`

+ eg:
```python
proxy = urllib.request.ProxyHandler({'http': 'ip'})  # 设置proxy
opener = urllib.request.build_opener(proxy)  # 挂载opener
urllib.request.install_opener(opener)  # 安装opener
data = urllib.parse.urlencode(data).encode('utf-8')#下载
```