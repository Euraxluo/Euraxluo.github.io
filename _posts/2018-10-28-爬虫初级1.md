---
layout:   post          
title:   爬虫学习1        
date:    2018-10-28      
author:   Euraxluo           
categories:Crawler
tags:  爬虫
---
* TOC
{:toc}


## 前记：
+ 爬虫：使用任何技术手段，批量获取网站信息的一种方式。关键在于批量。
+ 反爬虫：使用任何技术手段，阻止别人批量获取自己网站信息的一种方式。关键也在于批量。
+ 误伤：在反爬虫的过程中，错误的将普通用户识别为爬虫。误伤率高的反爬虫策略，效果再好也不能用。

+ 拦截：成功地阻止爬虫访问。这里会有拦截率的概念。通常来说，拦截率越高的反爬虫策略，误伤的可能性就越高。因此需要做个权衡。

+ 资源：机器成本与人力成本的总和。



## url 管理器：管理待抓取url集合和已抓取url集合



个人：set(),python的set()可以自动去重

大量带爬取url：关系数据库mysql

互联网公司：缓存数据库(高性能)



## 网页下载器：

### 1.urllib2：python官方基础模块（py2.7）

下载方法：

1.直接下载 

```python

import urllib2

response = urllib2.urlopen(url)#直接下载

print response.getcode()#获取状态码

cont = response.read()#读取内容

```

2.伪装和密码

```python 

import urllib2

request = urllib2.Request(url)#创建request对象

request.add_data('a','l')#添加数据，a-l,诸如账号密码

request.add_header('User-Agent','Mozilla/5.0')#添加http的header，用于伪装

response = urllib2.urlopen(request)#发送请求获取结果

cont = response.read()#读取内容

```

3.复杂情景（加套子）

HTTPCookie用户登录情景/Proxy代理信息/HTTPS加密信息/Readirect防止URL互相跳转

```python

import urllib2,cookielib

cj = cookielib.CookieJar()#创建cookie容器

opener = urllib2.builb_opener(urllib2.HTTPCookieProcessor(cj))#httpcookie用户登陆

urllib2.intall_opener(opener)#给urllib2安装opener

response = urllib2.urlopen(url)#使用带有cookie的urllib2爬取网页

```



### 2.urllib.request:(py3)

#### 2.1 request.urlopen方法： 

##### `urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None) `

+ urlopen无法判断数据的encoding，所以返回的是bytes对象。一般会对返回的数据进行decode。

+ url参数，可以是一个string，或者一个Request对象。

+ data一定是bytes对象，传递给服务器的数据，或者为None。目前只有HTTP requests会使用data，提供data时会是一个post请求，如若没有data，那就是get请求。data在使用前需要使用urllib.parse.urlencode()函数转换成流数据

 ##### urlopen方法：

-  read() , readline() ,readlines() , fileno() , close() ：对HTTPResponse类型数据进行操作 

-  info()：返回HTTPMessage对象，表示远程服务器返回的头信息 

-  getcode()：返回Http状态码。如果是http请求，200请求成功完成;404网址未找到 

-  geturl()：返回请求的url

##### eg:

```python

from urllib import request

req = request.urlopen('http://euraxluo.cn')

print(req.read().decode())#read()方法是读取返回数据内容，decode是转换返回数据的bytes格式为str 

```

#### 2.2 urllib.request.Reques：



`urllib.request.Request(url, data=None, headers={},origin_req_host=None, unverifiable=False, method=None)`

##### eg:

```python

import urllib.request

req=urllib.request.Request('http://euraxluo.cn')

with urllib.request.urlopen(req) as response:

    page=response.read(300).decode('utf-8')#我们获取的数据一般是ascii的，decode成utf-8.

print(page)

```



#### 2.3 用来包装头部的数据： 

+ User-Agent ：这个头部可以携带如下几条信息：浏览器名和版本号、操作系统名和版本号、默认语言 

```常用爬虫UA

百度图片

“Baiduspider-image+(+http://www.baidu.com/search/spider.htm)”



百度最新UA如下：

PC：

Mozilla/5.0 (compatible; Baiduspider-render/2.0; +http://www.baidu.com/search/spider.html)

移动：

Mozilla/5.0 (iPhone; CPU iPhone OS 9_1 like Mac OS X) AppleWebKit/601.1.46 (KHTML, like Gecko) Version/9.0 Mobile/13B143 Safari/601.1 (compatible; Baiduspider-render/2.0; +http://www.baidu.com/search/spider.html)



360搜索

Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0);



360网站安全检测

360spider (http://webscan.360.cn)



Google

“Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)”



Google图片搜索

“Googlebot-Image/1.0”



Adwords移动网络

“AdsBot-Google-Mobile (+http://www.google.com/mobile/adsbot.html) Mozilla (iPhone; U; CPU iPhone OS 3 0 like Mac OS X) AppleWebKit (KHTML, like Gecko) Mobile Safari”



微软 bing，必应

“Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)”



腾讯搜搜

“Sosospider+(+http://help.soso.com/webspider.htm)”



搜搜图片

“Sosoimagespider+(+http://help.soso.com/soso-image-spider.htm)”



雅虎英文

“Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)”



雅虎中国

“Mozilla/5.0 (compatible; Yahoo! Slurp China; http://misc.yahoo.com.cn/help.html)”



搜狗图片

“http://pic.sogou.com” “Sogou Pic Spider/3.0(+http://www.sogou.com/docs/help/webmasters.htm#07)”



搜狗

“Sogou web spider/4.0(+http://www.sogou.com/docs/help/webmasters.htm#07)”



网易有道

“Mozilla/5.0 (compatible; YoudaoBot/1.0; http://www.youdao.com/help/webmaster/spider/; )”



瑞典 Speedy Spider

“Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) Speedy Spider (http://www.entireweb.com/about/search_tech/speedy_spider/)”



俄罗斯 yandex

“Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)”



宜搜 EasouSpider

Mozilla/5.0 (compatible; EasouSpider; +http://www.easou.com/search/spider.html)



华为赛门铁克蜘蛛

“HuaweiSymantecSpider/1.0+DSE-support@huaweisymantec.com+(compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR ; http://www.huaweisymantec.com/cn/IRL/spider)”

华为赛门铁克科技有限公司网页信誉分析系统的一个页面爬取程序，其作用是用于爬取互联网网页并进行信誉分析，从而检查该网站上的是否含有恶意代码。



七牛镜像蜘蛛

qiniu-imgstg-spider-1.0



监控宝

“Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; JianKongBao Monitor 1.1)”



DNSPod监控

DNSPod-Monitor/2.0

```

+ Referer：可以用来防止盗链，有一些网站图片显示来源http://*.com，就是检查Referer来鉴定的 

+ Connection：表示连接状态，记录Session的状态

```

#urllib.request.Request(url, data=None, headers={}, method=None)

#使用request（）来包装请求，再通过urlopen（）获取页面。

```





#### 2.4 Post数据



+ urlopen()的data参数默认为None，当data参数不为空的时候，urlopen（）提交方式为Post

`urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)`

`

urlencode（）主要作用就是将url附上要提交的数据。



+ Post的数据必须是bytes或者iterable of bytes，不能是str，因此需要进行encode（）编码

page = request.urlopen(req, data=data).read()

当然，也可以把data的数据封装在urlopen（）参数中

`data = {'first': 'true',}`

`data = parse.urlencode(data).encode('utf-8')`



+ eg:

```python

import urllib.parse as up

import urllib.request as ur

url = 'http://www.uustv.com/'

values = {'word': 'Euraxluo','sizes': '60','fonts': 'jfcs.ttf','fontcolor': '#000000'}#Post的数据

data = up.urlencode(values)#编码

data = data.encode('ascii')#解码,server,只接受ascii数据

req = ur.Request(url,data)

with ur.urlopen(req) as response:

    page = response.read().decode('utf-8')  #我们获取的数据一般是ascii的，decode成utf-8.

print(page)

```

#### 使用代理

+ 当需要抓取的网站设置了访问限制，这时就需要用到代理来抓取数据

`urllib.request.ProxyHandler(proxies=None)`



+ eg:

```python

proxy = urllib.request.ProxyHandler({'http': 'ip'})  # 设置proxy

opener = urllib.request.build_opener(proxy)  # 挂载opener

urllib.request.install_opener(opener)  # 安装opener

data = urllib.parse.urlencode(data).encode('utf-8')#下载

```
