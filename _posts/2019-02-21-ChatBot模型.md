---
layout:     post                    # 使用的布局（不需要改）
title:      从零开始的ChatBot               # 标题 
subtitle:   ChatBot模型基础                #副标题
date:       2019-2-21            # 时间
author:     Euraxluo                      # 作者
header-img: img/post-bg-github-cup.jpg  #这篇文章标题背景图片
catalog: true                 # 是否归档
tags:                               #标签
    - NLP

---
# seq2seq
seq2seq是一个Encoder-Decoder结构的网络,它的输入是一个序列,输出也是一个序列
- Encoder中将一个可变长度的信号序列变为固定长度的向量表达
- Decoder将这个固定长度的向量变成可变长度的目标的信号序列
- 输入序列和输出序列的长度是可变的
- 可以用于翻译,聊天机器人,句法分析,文本摘要

![](/image/seq2seq2.png)
## encoder 过程
- 取得输入的文本,进行enbedding
- 传入到LSTM中进行训练
- 记录状态,并输出当前cell的结果
- 依次循环,得到最终结果
![](/image/encoder.png)
## decoder过程
- 在encoder最后一个时间步长的隐藏层之后输入到decoder的第一份cell里
- 通过激活函数得到候选的文本
- 筛选出可能性最大的文本作为下一个时间步长的输入
- 依次输入,得到目标
![](/image/decoder.png)

![decoder和encoder过程](/image/seq.png)
## 注意力机制
注意力机制是在序列到序列模型中用于注意编码器状态的最常用方法,它同时还可用于回顾序列模型的过去状态
- 注意力机制不仅能用来处理编码器或前面的隐藏层,它同样还能用来获得其他特征的分布
![](/image/attention.png)

### 为什么需要注意力机制
- 减小处理高纬度输入数据的计算负担,通过结构化的选取输入的子集,降低数据维度
- 让任务处理系统更专注于找到输入数据中显著的与当前输出相关的有用信息.从而提高输出的质量
- Attention模型的最终目的是帮助类似编解码器这样的框架,跟好的学到多种内容模态的相互关系,从而更好的表示这些信息,克服其无法解释从而很难设计的缺陷


![只有seq](/image/onlyseq.png)
![seq加上注意力机制](/image/attionAseq.png)
# 聊天机器人根据对话的产生方式
- 基于检索的模型
- 基于生成式的模型
## 基于检索的模型
- 在数据库中存储问答对
- 使用语句匹配的形式查找答案
- 答案相对固定,且很少出现语法错误
- 不会出现新的语句
![基于检索](/image/QAseq.png)
## 基于生成式模型
- 不依赖预先设定的问答库
- 通常基于机器翻译技术
- 需要大量的语料进行训练
- Encoder-Decoder模式
![](/image/e2dlstm.png)

## 混合模式
- 兼具检索模式和生成模式
- 检索模式产生候选数据集
- 生成模式产生最终答案

