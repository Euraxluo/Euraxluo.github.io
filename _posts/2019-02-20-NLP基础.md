---
layout:     post                    # 使用的布局（不需要改）
title:      从零开始的ChatBot               # 标题 
subtitle:   NLP基础                #副标题
date:       2019-2-20            # 时间
author:     Euraxluo                      # 作者
header-img: img/post-bg-github-cup.jpg  #这篇文章标题背景图片
catalog: true                 # 是否归档
tags:                               #标签
    - NLP

---
# NLP概览
## 什么是NLP
- 自然语言处理,是探讨如何处理及运用自然语言
- 自然语言认知,是让电脑明白人类的语言
- 自然语言处理主要包括:文本分析,信息检索,词性标注,问答系统QA

## NLP技术
>词法分析
	- 分词技术
	- 词性标注part-of-speech tagging
	- 命名实体识别NER(识别出3大类和7小类主要用于信息提取,QA,句法分析,机翻元数据标注)
		实体边界识别
		基于规则和词典进行识别(字典大小,新词?)
		基于统计的方法
		>隐马尔科夫模型HMM
		>较大熵ME
		>支持向量机SVM
		>条件随机场CRF
		确定实体类别(英文,中文需要先分词)
	- 词义消歧

>句法分析

> 语义分析

## 常见模型
- 传统感知机模型

- BP神经网络:前馈神经网络(反向传播算法),是现代神经网络的基础
  ![](/image/bp.png)
  输入层:数据模型的输入,也就是说我们要传入到模型中的数据
  隐藏层:用于处理数据,并将处理的结果传递给输出层
  输出层:经过隐藏层的计算过后输出的模型内容,分类信息,或者是模型的最终参数
  训练过程概述:

  1. 正向传播:网络初始化(定义网络参数),隐藏层的输出,输出层的输出
  2. 误差计算:通过误差计算的公式,计算出误差
  3. 反向传播:通过计算的误差,从输出层向后传播,并在过程中更新权重参数 
  4. 偏置更新:通过计算的误差,更新隐藏层到输出层,输入层到隐藏层的权重参数 
  特点:
  - 可以通过逐层信息传递到最后的输出
  - 沿着一条直线计算,直到最后一层,求出计算结果
  - 包含输入层,输出层和隐藏层,目的是实现输入到输出的映射
  - 一般包含多层,并且层与层之间是全链接的不会出现同层和跨层连接

- CNN:是一种前馈神经网络,包括卷积层(convolutional)和池化层(pooling layer)

- RNN:循环神经网络是一种节点定向连接成环的人工神经网络,这种网络的内部状态可以动态的展示时序行为(短文本)
  ![](/image/RNN.png)
  特点:记忆特性;接受两个参数W和当前时间的特征;参数共享(确保每一步都在做相同的事)
  网络结构和BP神经网络的对比:
  ![](/image/bptornn.png)
  RNN的类型:
  ![](/image/rnntype.png)

  - one to one:适合用于分类任务
  - one to many:文本生成,音乐生成
  - many to one:多分类任务
  - many to many(不同维度):翻译任务
  - many to many(同维度):命名实体识别

- LSTM:长短期记忆网络,是一种时间递归神经网络.适合于处理和预测时间序列中间隔和延迟相对较长的重要事件(长文本)
	在普通的RNN中增加了一种由门控制的保存单元状态的结构:c
	![](/image/lstm.png)
	通过遗忘门,输出门,输入门
- GRU
	![](/image/gru.png)
	只要更新门和重置门,没有隐藏层(可能不太关注时序的各种关系???我不太懂)
	
- 双向循环神经网络:
  ![](/image/fbrnn.png)
  特点:

  - 每个时刻有两个隐藏层
  - 一个Forward Layer;另一个Backward Layer
  - 向前传播和向后传播的参数是独立的

##　梯度消失和梯度爆炸
在训练RNN中最常见的问题
![](/image/dnone.png)
解决方法:

- 选择合适的激活函数
	>ReLu函数(最常使用)
	>Sigmod函数和Tanh函数(这两个的导数在大部分区域很小,容易产生梯度消散)
- 选择合适的参数初始化方法不能设置为0 
	>权重参数=`np.random.randn(w的shape)*0.01`(适用于小任务,解决参数对称)
	>权重参数=`np.random.randn(w的shape)*np.sqrt(1/(上一层的神经元数))`(主要适用于ReLu激活函数,可以缓解梯度消散)
- 使用权重参数正则化
- 使用BatchNormailzation
	>通过规范化的操作将输出信号x规范化到均值为0,方差为1保证网络的稳定性(把偏离的参数规范到高斯分布上)
	>可以加大神经网络的训练速度
	>提高训练的稳定性
	>缓解梯度抱着和梯度消散的问题
- 使用残差网络
	>在神经网络中加入以下结构:
	![](/image/canca.png)
	>通过跨层连接,使得快速下降的参数能得到缓解,让神经网络的深度大大提高,同时解决了梯度消失的问题
- 使用梯度裁剪
	>强制的让我们的梯度变小
	>算法:$if    ||g||>v ;then    g<-- gv/(||g||)$
	>在可视化的层面上将,就是让我们的导数不去跨越面,温和的梯度下降

## 隐马尔可夫模型实现命名实体识别
### 马尔科夫过程
- 马尔科夫过程(Markov process)是一类随机过程
- 在已知目前状态(现在)的条件下,它未来的演变(将来)不依赖于它以往的演变(过去).主要研究一个系统的状态及其转移的理论.他是通过对不同状态的初始概率以及状态的转移概率的研究,来确定状态的变化趋势,从而达到预测未来的目的
### 马尔科夫链(Markov chain)
- 是指具有马尔科夫性质的离散事件随机过程,即时间和状态参数都是离散的马尔科夫过程,是最简单的马尔科夫过程
### 隐马尔可夫模型(Hidden Markov Model,HMM)
- 一种统计分析模型,是马尔科夫链的一种,它的状态不能被直接观察到,但能通过观测向量序列观察到,每个观测向量都是通过某些概率密度分布变现为个各种状态,每一个观测向量是由一个具有响应概率密度分布的状态序列产生
- 是结构最简单的动态贝叶斯网(一种有向图模型),主要用于时序数据建模(语音识别,自然语言处理)
- 隐马尔可夫模型由五个要素组成,其中两个状态集合(N.M),三个概率矩阵(A.B,π)
	1) N:表示模型中的状态数,状态之间可以相互转移
	2) M:表示每个状态不同的观察符号,即输出字符的个数
	3) A:状态转移概率
	4) B:观察符号在各个状态下的概率分布
	5) π:表示初始状态分布
- 隐马尔可夫模型的输入和输出
	输入:HMMs的五元组(N,M,A,B,π)
	输出:一个观察符号的序列,这个序列的每个元素都是M中的元素
### 使用隐马尔科夫模型实现命名实体识别
![](/image/HMMs.png)
- 训练:通过语料进行训练,输出概率用于NE识别,这当中用了大量的贝叶斯
- NE识别:给定各种状态下不同分词的概率以及完成人工词性标注的句子求出词性标注概率最大的状态
- 规则修正:对于一些特殊名词的标注进行规则修正
- 标注转换:通过序列处理把实现NE标注多个词复合,求得整个序列串概率最大的标注方案


## 语料
### 语料库
- 语言材料.语料是语言学研究的内容.语料是构成语料库的基本单元
- 语料库中存放的是在语言的实际使用中真实出现的语言材料
- 语料库是以电子计算机为载体承载语言知识的基础资源
- 真实语料需要经过加工(分析和处理),才能成为有用的资源

### 语料库的种类
- 异质
- 同质
- 系统
- 专用

### 获取途径
- 爬虫
- 平台

### 语料的处理
1. 获取语料
2. 格式化文本
3. 特征工程 

## NLP中的语言模型
语言模型是自然语言处理中的一个利器,是NLP领域一个基本又重要的任务.它的主要更能是计算一个词语序列构成一个句子的概率,或者说计算一个词语序列的联合概率,这可以用来判断一句话出现的概率高不高,是否符合我们的表达习惯,这句话是否正确

### 概率语言模型

预测字符串概率,考虑动机,考虑计算方式

- Unigram models(一元文法统计模型)
- N-gram 语言模型(N元模型)

### 一元文法统计模型 
p(s) = p(w1)*p(w2)*p(w3)*p(w4)*p(w5)*p(w6)*p(w7)
我们假设每个词都条件无关
### 二元文法统计模型
p(s) = p(w1|</s>)*p(w2|w1)*p(w3|w2)*...*p(</s>|wn)
二元语言模型可以比一元语言模型更考虑到两个词之间的关系信息
### N元模型
$P( w1,w2,...,w_m) = i...m(*) P(w_i|w1,...,w_(i-1)) = i...m(*) P(w_i|w_(i-n+1),...,w_(i-1))$
### 注:
n大于3时基本无法处理,参数空间太大.另外它不能表示词与词之间的关联性


## 词向量(Word embedding)
即词嵌入,是自然语言处理中的一组语言建模和特征学习技术的统称,其中来自词汇表的单词或短语被映射到实数的向量

### Word2vec
是为一群用来产生词向量的相关模型.这些模型为浅而双层的神经网络,用来训练以重新构建语言学之词文本
-  CBOW
>CBOW模型由输入层,映射层,输出层共同构成
>CBOW所构建的模型结构实际上是一个二叉树结构,应用到Word2vec中被称为Hierarchical Softmax

- Skip-gram
>Skip-Gram模型由输入层,映射层,输出层共同构成
>Skip-Gram所构建的模型结构实际上是一个二叉树结构,并且刚好和CBOW模型相反

## 文本处理方法
- 数据清洗(去掉无意义的标签,url,符号等)
- 分词,大小写转换,添加句首句尾,词性标注
- 统计词频,抽取文本个特征,特征选择,计算特征权重,归一化 
- 划分训练集,测试集
