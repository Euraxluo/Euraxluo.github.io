<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on Euraxluo Blog</title>
    <link>https://euraxluo.github.io/tags/python/</link>
    <description>Recent content in Python on Euraxluo Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Copyright © 2016-{year} Euraxluo. All Rights Reserved.</copyright>
    <lastBuildDate>Thu, 21 Feb 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://euraxluo.github.io/tags/python/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>python socket编程</title>
      <link>https://euraxluo.github.io/posts/python/python-socket/</link>
      <pubDate>Thu, 21 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://euraxluo.github.io/posts/python/python-socket/</guid>
      <description>TCP的三次握手和四次挥手
三次握手: 为什么需要三次握手? 客户端:我可以发东西给你(确保客户端的发送能力) 服务器:我可以收到,你能收到么?(确保服务器的接受和发送) 客户端:我能收到!(确保能收到)
连接建立!
如果是四次握手? 没必要啊,第三次已经确认可以收到消息了
如果是两次握手? 当网络阻塞时,客户端会发送两次,第一次请求到达服务器的时间慢于第二次 如果当时通信结束,服务器又收到了第一次阻塞的消息,如果是两次握手,就会分配资源 然而客户端已经完成了通信,不需要再连接了,会造成资源的浪费和安全隐患
四次挥手: 客户端:我说完了,我想停止发送请求了 服务器:我知道你要停止发送了,我会停止接受消息 ( 服务器停止接受消息,但是可能还有很多待发送的消息
客户端:收到服务器的确认信息,于是默不作声,等待服务器发送完他的消息
) 服务器:我的东西全发完啦!,我想要停止发送消息啦! 客户端:我知道你也要停止发送了,我也要停止接收消息(实际上还等了两个最大周期才真正停止接收消息) ( 服务器:收到了客户端的确认消息,于是停止发送消息 )
关于tcp的博客 使用tcp和udp让进程之间进行通信
ip地址：用來標記網絡上的主機 動態端口：1024-65535的端口，用完就回收
tcp socket client的基本流程
import socket ##創建socket s = socket.socket(socket.af_inet,socket.sock_stream) ##使用 ipaddr = (&amp;#34;ip&amp;#34;,port)#服务器的ip addr s.connect(ipaddr)#连接服务器 ### 发送数据 send_msg = &amp;#34;sasa&amp;#34; s.send(send_msg.encode(&amp;#34;utf-8&amp;#34;)) ### 接受数据 recvData = s.rec(1024)#一次接收的字符数 print(&amp;#34;recved msg:&amp;#34;,recvData.decode(&amp;#34;&amp;#34;utf-8)) ##關閉 s.close() tcp server的基本过程
# socket创建套接字 tcp = socket.socket(socket.AF_INET,socket.SOCK_STREAM) # 绑定端口 tcp.bind((&amp;#34;127.1&amp;#34;,7788)) # 设置为被动监听 tcp.</description>
    </item>
    
    <item>
      <title>Flask入门</title>
      <link>https://euraxluo.github.io/posts/python/flask%E5%85%A5%E9%97%A8/</link>
      <pubDate>Thu, 14 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://euraxluo.github.io/posts/python/flask%E5%85%A5%E9%97%A8/</guid>
      <description>Flask 学习 入门： 最小的Flask 程序 from flask import Flask # 导入flask app = Flask(__name__)#使用单一的模块（如本例），你应该使用 __name__ @app.route(&amp;#39;/hello&amp;#39;) #route()装饰器 什么样子的URL能触发我们的函数 def hello_word(): return &amp;#39;Hello Word!&amp;#39;#返回我们想在浏览器中显示的内容 if __name__ == &amp;#39;__main__&amp;#39;: #确保服务器只会在该脚本被 Python 解释器直接执行的时候才会运行，而不是作为模块导入的时候 #app.run()#让app这个应用在本地服务器运行起来 #app.run(host=&amp;#39;0.0.0.0&amp;#39;) #监听所有的公网IP app.debug = True app.run()#q启动调试器 模板渲染 Jinja2模板引擎文档
from flask import Flask # 导入flask from flask import render_template #使用Jinja2的模版渲染 app = Flask(__name__)#使用单一的模块（如本例），你应该使用 __name__ @app.route(&amp;#39;/hello&amp;#39;) #route()装饰器 什么样子的URL能触发我们的函数 def hello_word(): return render_template(&amp;#34;hello.html&amp;#34;)#返回的模板文件（需要放在当前目录的templates文件夹内） if __name__ == &amp;#39;__main__&amp;#39;: #确保服务器只会在该脚本被 Python 解释器直接执行的时候才会运行，而不是作为模块导入的时候 #app.run()#让app这个应用在本地服务器运行起来 #app.run(host=&amp;#39;0.0.0.0&amp;#39;) #监听所有的公网IP app.</description>
    </item>
    
    <item>
      <title>Python构建开源项目</title>
      <link>https://euraxluo.github.io/posts/python/python%E6%9E%84%E5%BB%BA%E5%BC%80%E6%BA%90%E8%BD%AF%E4%BB%B6/</link>
      <pubDate>Mon, 29 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://euraxluo.github.io/posts/python/python%E6%9E%84%E5%BB%BA%E5%BC%80%E6%BA%90%E8%BD%AF%E4%BB%B6/</guid>
      <description>Python构建开源软件 python的构建工具setup.py的应用场景 一般在安装python模块的时候,我们会使用pip install 模块名进行在线安装,会安装依赖包,或者python setup.py install通过源码在本地安装,不会安装依赖包
在做一个开源项目的时候遇到了一些问题: 我的程序需要用到python的Redis等模块,以及自己写的入口文件run.py,怎么实现可以在服务器上方便的发布,也就是说,可以让依赖和自己写的程序一起安装,同时将自己写的模块变成一个可执行文件
###　setup.py
示例以及注释:
from setuptools import setup, find_packages setup( name = &amp;#34;proxy-pool&amp;#34;, #包名 version = &amp;#34;1.0.0&amp;#34;, #版本 keywords = (&amp;#34;poxypool&amp;#34;, &amp;#34;redis&amp;#34;),#关键词列表 description = &amp;#34;test version proxy pool&amp;#34;, #程序的简单介绍 long_description = &amp;#34;A proxy pool project modified from Germey/ProxyPool&amp;#34;, #程序的详细介绍 url = &amp;#34;https://github.com/Euraxluo/ProxyPool&amp;#34;, #程序的官网  download_url = &amp;#34;https://github.com/Euraxluo/ProxyPool.git&amp;#34; #程序的下载地址 author = &amp;#34;Euraxluo&amp;#34;, #作者 author_email = &amp;#34;euraxluo@qq.com&amp;#34;, #程序作者的邮箱 #maintainer 维护者 #maintainer_email 维护者的邮箱地址 packages=[ &amp;#39;proxy-pool&amp;#39; ], py_modules = [&amp;#39;run&amp;#39;],#需要打包的python文件列表 include_package_data = True, platforms = &amp;#34;any&amp;#34;, #程序适用的软件平台列表 install_requires = [#需要安装的依赖包 &amp;#39;aiohttp&amp;#39;, &amp;#39;requests&amp;#39;, &amp;#39;flask&amp;#39;, &amp;#39;redis&amp;#39;, &amp;#39;pyquery&amp;#39; ], entry_points = { #动态发现服务和插件 &amp;#39;console_scripts&amp;#39;: [ #指定命令行工具的名称 &amp;#39;test = test.</description>
    </item>
    
    <item>
      <title>RabbitMQ-入门</title>
      <link>https://euraxluo.github.io/posts/message_queue/rabbitmq_%E5%85%A5%E9%97%A8/</link>
      <pubDate>Mon, 22 Oct 2018 00:00:00 +0800</pubDate>
      
      <guid>https://euraxluo.github.io/posts/message_queue/rabbitmq_%E5%85%A5%E9%97%A8/</guid>
      <description>RbbitMQ 学习笔记 AMQP协议组成部分
 Module layer：协议最高层，定义了供客户端使用的命令 Session layer：中间层，负责将客户端的命令发送给服务端，再将服务端的命令返回给客户端，为客户端和服务端之间提供可靠的通信 Transport layer：最底层，包括二进制流的传输，帧处理，信道复用，错误检测  生产者使用AMQP的过程  Producter   建立连接 开启通道 发送消息 释放资源  消费者使用AMQP的过程  Consumer   建立连接 开启通道 准备接受消息 发送确认 释放资源  AMQP命令和javaAPI的对应 Connection.Start : factory.newConnection 新建连接 Connection.close : connection.close 关闭连接 Channel.Open : channel.openChannel 开启信道 Channel.close : channel.close 关闭信道 Exchange.Declare : channel.exchangeDeclare 声明交换器 Exchange.Delete : channel.exchangeDelete删除交换器 Exchange.Bind : channel.exchangeBind 交换器和交换器绑定 Exchange.Unbind : channel.exchangeUnbind 交换器和交换器解绑 Queue.Declare : channel.queueDeclare 声明队列 Queue.Bind : channel.</description>
    </item>
    
    <item>
      <title>爬虫学习1-概念及urllib2</title>
      <link>https://euraxluo.github.io/posts/crawler/%E7%88%AC%E8%99%AB%E5%88%9D%E7%BA%A71/</link>
      <pubDate>Mon, 22 Oct 2018 00:00:00 +0800</pubDate>
      
      <guid>https://euraxluo.github.io/posts/crawler/%E7%88%AC%E8%99%AB%E5%88%9D%E7%BA%A71/</guid>
      <description>前记：   爬虫：使用任何技术手段，批量获取网站信息的一种方式。关键在于批量。
  反爬虫：使用任何技术手段，阻止别人批量获取自己网站信息的一种方式。关键也在于批量。
  误伤：在反爬虫的过程中，错误的将普通用户识别为爬虫。误伤率高的反爬虫策略，效果再好也不能用。
  拦截：成功地阻止爬虫访问。这里会有拦截率的概念。通常来说，拦截率越高的反爬虫策略，误伤的可能性就越高。因此需要做个权衡。
  资源：机器成本与人力成本的总和。
  url 管理器：管理待抓取url集合和已抓取url集合 个人：set(),python的set()可以自动去重
大量带爬取url：关系数据库mysql
互联网公司：缓存数据库(高性能)
网页下载器： 1.urllib2：python官方基础模块（py2.7） 下载方法：
1.直接下载
import urllib2 response = urllib2.urlopen(url)#直接下载 print response.getcode()#获取状态码 cont = response.read()#读取内容 2.伪装和密码
import urllib2 request = urllib2.Request(url)#创建request对象 request.add_data(&amp;#39;a&amp;#39;,&amp;#39;l&amp;#39;)#添加数据，a-l,诸如账号密码 request.add_header(&amp;#39;User-Agent&amp;#39;,&amp;#39;Mozilla/5.0&amp;#39;)#添加http的header，用于伪装 response = urllib2.urlopen(request)#发送请求获取结果 cont = response.read()#读取内容 3.复杂情景（加套子）
HTTPCookie用户登录情景/Proxy代理信息/HTTPS加密信息/Readirect防止URL互相跳转
import urllib2,cookielib cj = cookielib.CookieJar()#创建cookie容器 opener = urllib2.builb_opener(urllib2.HTTPCookieProcessor(cj))#httpcookie用户登陆 urllib2.intall_opener(opener)#给urllib2安装opener response = urllib2.urlopen(url)#使用带有cookie的urllib2爬取网页 2.urllib.request:(py3) 2.1 request.urlopen方法： urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)    urlopen无法判断数据的encoding，所以返回的是bytes对象。一般会对返回的数据进行decode。</description>
    </item>
    
    <item>
      <title>爬虫学习2-Requests库学习</title>
      <link>https://euraxluo.github.io/posts/crawler/%E7%88%AC%E8%99%AB%E5%88%9D%E7%BA%A72/</link>
      <pubDate>Mon, 22 Oct 2018 00:00:00 +0800</pubDate>
      
      <guid>https://euraxluo.github.io/posts/crawler/%E7%88%AC%E8%99%AB%E5%88%9D%E7%BA%A72/</guid>
      <description>请求方法： r=requests.get(&amp;#39;http://httpbin.org/get&amp;#39;)#get r = requests.post(&amp;#34;http://httpbin.org/post&amp;#34;)#post r = requests.put(&amp;#34;http://httpbin.org/put&amp;#34;)#put r = requests.delete(&amp;#34;http://httpbin.org/delete&amp;#34;)#delect r = requests.head(&amp;#34;http://httpbin.org/get&amp;#34;)#head r = requests.options(&amp;#34;http://httpbin.org/get&amp;#34;)#options GET eg import requests r = requests.get(url=&amp;#39;http://www.euraxluo.cn&amp;#39;) # 最基本的GET请求 print(r.status_code) # 内置的状态码查询对象 #状态码非200视为出错 响应状态码 eg:404 r = requests.get(&amp;#39;http://httpbin.org/status/404&amp;#39;) print(r.status_code)#404 error_info = r.raise_for_status()#Response.raise_for_status()抛出异常 带参数的url请求： #向url传递参数 r = requests.get(url=&amp;#39;http://dict.baidu.com/s&amp;#39;, params={&amp;#39;wd&amp;#39;: &amp;#39;python&amp;#39;})#带参数的GET请求 #当你不知道你的编码类型时 r.encoding = r.apparent_encoding#获取编码类型 print(r.text)#返回解码后的数据 tips 若有图片 r.content 返回bytes数据
eg：r.content r = requests.get(url=&amp;#39;http://music.baidu.com&amp;#39;)#实测，没啥区别 html=r.content #html_doc=str(html,&amp;#39;utf-8&amp;#39;) html_doc=html.decode(&amp;#34;utf-8&amp;#34;,&amp;#34;ignore&amp;#34;) print(html_doc) 响应内容 不同的内容处理方式 Json：request.json() 二进制：一般用于图片 from PIL import Image from io import BytesIO m = request.</description>
    </item>
    
    <item>
      <title>爬虫学习3-网页解析器</title>
      <link>https://euraxluo.github.io/posts/crawler/%E7%88%AC%E8%99%AB%E5%88%9D%E7%BA%A73/</link>
      <pubDate>Mon, 22 Oct 2018 00:00:00 +0800</pubDate>
      
      <guid>https://euraxluo.github.io/posts/crawler/%E7%88%AC%E8%99%AB%E5%88%9D%E7%BA%A73/</guid>
      <description>BeautifulSoup解析器：    解析器 使用方法 条件     bs4的html解析器 BeautifulSoup(html,&amp;lsquo;html.parser&amp;rsquo;) 安装bs4   lxml的html解析器 BeautifulSoup(html,&amp;lsquo;lxml&amp;rsquo;) pip install lxml   lxml的xml解析器 BeautifulSoup(html,&amp;lsquo;xml&amp;rsquo;) pip install lxml   html5lib的解析器 BeautifulSoup(html,&amp;lsquo;html5lib&amp;rsquo;) pip install html5lib    基本元素    基本元素 说明     tag 标签,&amp;lt;&amp;gt;开头，&amp;lt;/&amp;gt;结尾   name 标签的名字，,&amp;lt;tag&amp;gt;.name   attrs 标签的属性，&amp;lt;tag&amp;gt;.attrs   NavigableString String,&amp;lt;tag&amp;gt;.String   Comment 标签内字符串的注释部分，Comment类型    搜索节点(html中的标签) find_all(name,attrs,recursive,string)</description>
    </item>
    
  </channel>
</rss>
