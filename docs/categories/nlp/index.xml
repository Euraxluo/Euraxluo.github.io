<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLP on Euraxluo Blog</title>
    <link>https://euraxluo.github.io/categories/nlp/</link>
    <description>Recent content in NLP on Euraxluo Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Copyright © 2016-{year} Euraxluo. All Rights Reserved.</copyright>
    <lastBuildDate>Thu, 22 Oct 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://euraxluo.github.io/categories/nlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>NLP基础 </title>
      <link>https://euraxluo.github.io/posts/nlp/nlp%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Thu, 22 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://euraxluo.github.io/posts/nlp/nlp%E5%9F%BA%E7%A1%80/</guid>
      <description>NLP概览 什么是NLP   自然语言处理,是探讨如何处理及运用自然语言
  自然语言认知,是让电脑明白人类的语言
  自然语言处理主要包括:文本分析,信息检索,词性标注,问答系统QA
  NLP技术  词法分析
 - 分词技术- 词性标注part-of-speech tagging- 命名实体识别NER(识别出3大类和7小类主要用于信息提取,QA,句法分析,机翻元数据标注)实体边界识别基于规则和词典进行识别(字典大小,新词?)基于统计的方法&amp;gt;隐马尔科夫模型HMM&amp;gt;较大熵ME&amp;gt;支持向量机SVM&amp;gt;条件随机场CRF确定实体类别(英文,中文需要先分词)- 词义消歧  句法分析
  语义分析
 常见模型   传统感知机模型
  BP神经网络:前馈神经网络(反向传播算法),是现代神经网络的基础
输入层:数据模型的输入,也就是说我们要传入到模型中的数据
隐藏层:用于处理数据,并将处理的结果传递给输出层
输出层:经过隐藏层的计算过后输出的模型内容,分类信息,或者是模型的最终参数
训练过程概述:
  正向传播:网络初始化(定义网络参数),隐藏层的输出,输出层的输出
  误差计算:通过误差计算的公式,计算出误差
  反向传播:通过计算的误差,从输出层向后传播,并在过程中更新权重参数
  偏置更新:通过计算的误差,更新隐藏层到输出层,输入层到隐藏层的权重参数
  特点:
  可以通过逐层信息传递到最后的输出</description>
    </item>
    
    <item>
      <title>ChatBot模型基础</title>
      <link>https://euraxluo.github.io/posts/nlp/chatbot%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Thu, 21 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://euraxluo.github.io/posts/nlp/chatbot%E6%A8%A1%E5%9E%8B/</guid>
      <description>word2vec !(http://www.cnblogs.com/neopenx/p/4571996.html)(是个巨佬) !(https://blog.csdn.net/itplus/article/details/37969817)
概率语言模型 概率语言模型 预测字符串概率,考虑动机,考虑计算方式
  Unigram models(一元文法统计模型)
  N-gram 语言模型(N元模型
  N元模型 $P( w1,w2,&amp;hellip;,w_m) = i&amp;hellip;m(*) P(w_i|w1,&amp;hellip;,w_(i-1)) = i&amp;hellip;m(*) P(w_i|w_(i-n+1),&amp;hellip;,w_(i-1))$
注: n大于3时基本无法处理,参数空间太大.另外它不能表示词与词之间的关联性
神经概率语言模型 在论文《A neural probabilistic language model》中提出的一种模型.该模型的重要工具是词向量
词向量: 对词典D中的任意词w,指定一个固定长度的实值向量$v(w)\in R^m$v(w)就称为w的词向量,m为词向量的长度
概述 训练样本: (Context(w),w) 包括前n-1个词分别的向量,假定每个词向量大小m
投影层： (n-1)*m 首尾拼接起来的大向量
输出: 输出是一棵二叉树,它以语料中出现过的词当做叶子节点.以各词在语料中的出现次数当做权值垢找出来的Huffman树
y_w = (y_w1,y_w2,y_w3,&amp;hellip;y_wN,)
表示上下文为Context(w)时,下一个词恰好为词典中的第i个词的概率
归一化: $$p(w|Context(w)) = \frac{e^{yw,iw}}{\sum^{N}_{i=1}e^{yw,iw} }$$
哈弗曼树 最优二叉树,节点会有权重,指示词的常用频次
使用哈弗曼树
把高频词放在距离根节点近的地方,在测试时,我们每次预测每一层的正概率和负概率
CBOW 根据上下文的词语预测当前词语的出现概率的模型
$$L = \sum_{w\in c}logp(w|Context(w))$$
词向量=哈弗曼编码,经过不断地训练,哈弗曼编码不断改变
权重参数=通过每层的概率计算,最终指向这个词会有一个似然函数,其中的某个参数,就是sigmod函数中的theta
对最终的似然函数求最大==&amp;gt;最大化问题&amp;ndash;&amp;gt;梯度上升
Negative Sampling 为了解决,树空间过大</description>
    </item>
    
    <item>
      <title>语料处理基础</title>
      <link>https://euraxluo.github.io/posts/nlp/%E8%AF%AD%E6%96%99%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B/</link>
      <pubDate>Thu, 21 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://euraxluo.github.io/posts/nlp/%E8%AF%AD%E6%96%99%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B/</guid>
      <description>语料处理流程 语料收集 &amp;gt; 语料清洗 &amp;gt; 句子向量编码化 &amp;gt; 语料问答对构建 &amp;gt; 语料的模型保存 &amp;gt; 结束 语料收集   聊天记录
  电影对话
  台词片断
  语料清洗  要清洗的内容
   多余的空格
  不正规的符号
  多余的字符,英文
   清洗的方法
   正则化
  切分
  好坏语句判断
  语料问答对的构建  问答对的处理和拆分  句子向量的编码化   原始文本不能直接训练
  将句子转化为向量
  将向量转换为句子
  语料模型的保存   使用pickle来保存模型</description>
    </item>
    
  </channel>
</rss>
