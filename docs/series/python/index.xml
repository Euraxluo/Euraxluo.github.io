<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on Euraxluo Blog</title>
    <link>https://euraxluo.gitee.io/blog/series/python/</link>
    <description>Recent content in Python on Euraxluo Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Copyright © 2016-{year} Euraxluo. All Rights Reserved.</copyright>
    <lastBuildDate>Mon, 29 Oct 2018 00:00:00 +0000</lastBuildDate><atom:link href="https://euraxluo.gitee.io/blog/series/python/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Python构建开源项目</title>
      <link>https://euraxluo.gitee.io/blog/posts/python/python%E6%9E%84%E5%BB%BA%E5%BC%80%E6%BA%90%E8%BD%AF%E4%BB%B6/</link>
      <pubDate>Mon, 29 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://euraxluo.gitee.io/blog/posts/python/python%E6%9E%84%E5%BB%BA%E5%BC%80%E6%BA%90%E8%BD%AF%E4%BB%B6/</guid>
      <description>Python构建开源软件 python的构建工具setup.py的应用场景 一般在安装python模块的时候,我们会使用pip install 模块名进行在线安装,会安装依赖包,或者python setup.py install通过源码在本地安装,不会安装依赖包
在做一个开源项目的时候遇到了一些问题: 我的程序需要用到python的Redis等模块,以及自己写的入口文件run.py,怎么实现可以在服务器上方便的发布,也就是说,可以让依赖和自己写的程序一起安装,同时将自己写的模块变成一个可执行文件
###　setup.py
示例以及注释:
from setuptools import setup, find_packages setup( name = &amp;#34;proxy-pool&amp;#34;, #包名 version = &amp;#34;1.0.0&amp;#34;, #版本 keywords = (&amp;#34;poxypool&amp;#34;, &amp;#34;redis&amp;#34;),#关键词列表 description = &amp;#34;test version proxy pool&amp;#34;, #程序的简单介绍 long_description = &amp;#34;A proxy pool project modified from Germey/ProxyPool&amp;#34;, #程序的详细介绍 url = &amp;#34;https://github.com/Euraxluo/ProxyPool&amp;#34;, #程序的官网  download_url = &amp;#34;https://github.com/Euraxluo/ProxyPool.git&amp;#34; #程序的下载地址 author = &amp;#34;Euraxluo&amp;#34;, #作者 author_email = &amp;#34;euraxluo@qq.com&amp;#34;, #程序作者的邮箱 #maintainer 维护者 #maintainer_email 维护者的邮箱地址 packages=[ &amp;#39;proxy-pool&amp;#39; ], py_modules = [&amp;#39;run&amp;#39;],#需要打包的python文件列表 include_package_data = True, platforms = &amp;#34;any&amp;#34;, #程序适用的软件平台列表 install_requires = [#需要安装的依赖包 &amp;#39;aiohttp&amp;#39;, &amp;#39;requests&amp;#39;, &amp;#39;flask&amp;#39;, &amp;#39;redis&amp;#39;, &amp;#39;pyquery&amp;#39; ], entry_points = { #动态发现服务和插件 &amp;#39;console_scripts&amp;#39;: [ #指定命令行工具的名称 &amp;#39;test = test.</description>
    </item>
    
    <item>
      <title>爬虫学习1-概念及urllib2</title>
      <link>https://euraxluo.gitee.io/blog/posts/crawler/%E7%88%AC%E8%99%AB%E5%88%9D%E7%BA%A71/</link>
      <pubDate>Mon, 22 Oct 2018 00:00:00 +0800</pubDate>
      
      <guid>https://euraxluo.gitee.io/blog/posts/crawler/%E7%88%AC%E8%99%AB%E5%88%9D%E7%BA%A71/</guid>
      <description>前记：   爬虫：使用任何技术手段，批量获取网站信息的一种方式。关键在于批量。
  反爬虫：使用任何技术手段，阻止别人批量获取自己网站信息的一种方式。关键也在于批量。
  误伤：在反爬虫的过程中，错误的将普通用户识别为爬虫。误伤率高的反爬虫策略，效果再好也不能用。
  拦截：成功地阻止爬虫访问。这里会有拦截率的概念。通常来说，拦截率越高的反爬虫策略，误伤的可能性就越高。因此需要做个权衡。
  资源：机器成本与人力成本的总和。
  url 管理器：管理待抓取url集合和已抓取url集合 个人：set(),python的set()可以自动去重
大量带爬取url：关系数据库mysql
互联网公司：缓存数据库(高性能)
网页下载器： 1.urllib2：python官方基础模块（py2.7） 下载方法：
1.直接下载
import urllib2 response = urllib2.urlopen(url)#直接下载 print response.getcode()#获取状态码 cont = response.read()#读取内容 2.伪装和密码
import urllib2 request = urllib2.Request(url)#创建request对象 request.add_data(&amp;#39;a&amp;#39;,&amp;#39;l&amp;#39;)#添加数据，a-l,诸如账号密码 request.add_header(&amp;#39;User-Agent&amp;#39;,&amp;#39;Mozilla/5.0&amp;#39;)#添加http的header，用于伪装 response = urllib2.urlopen(request)#发送请求获取结果 cont = response.read()#读取内容 3.复杂情景（加套子）
HTTPCookie用户登录情景/Proxy代理信息/HTTPS加密信息/Readirect防止URL互相跳转
import urllib2,cookielib cj = cookielib.CookieJar()#创建cookie容器 opener = urllib2.builb_opener(urllib2.HTTPCookieProcessor(cj))#httpcookie用户登陆 urllib2.intall_opener(opener)#给urllib2安装opener response = urllib2.urlopen(url)#使用带有cookie的urllib2爬取网页 2.urllib.request:(py3) 2.1 request.urlopen方法： urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)    urlopen无法判断数据的encoding，所以返回的是bytes对象。一般会对返回的数据进行decode。</description>
    </item>
    
    <item>
      <title>爬虫学习2-Requests库学习</title>
      <link>https://euraxluo.gitee.io/blog/posts/crawler/%E7%88%AC%E8%99%AB%E5%88%9D%E7%BA%A72/</link>
      <pubDate>Mon, 22 Oct 2018 00:00:00 +0800</pubDate>
      
      <guid>https://euraxluo.gitee.io/blog/posts/crawler/%E7%88%AC%E8%99%AB%E5%88%9D%E7%BA%A72/</guid>
      <description>请求方法： r=requests.get(&amp;#39;http://httpbin.org/get&amp;#39;)#get r = requests.post(&amp;#34;http://httpbin.org/post&amp;#34;)#post r = requests.put(&amp;#34;http://httpbin.org/put&amp;#34;)#put r = requests.delete(&amp;#34;http://httpbin.org/delete&amp;#34;)#delect r = requests.head(&amp;#34;http://httpbin.org/get&amp;#34;)#head r = requests.options(&amp;#34;http://httpbin.org/get&amp;#34;)#options GET eg import requests r = requests.get(url=&amp;#39;http://www.euraxluo.cn&amp;#39;) # 最基本的GET请求 print(r.status_code) # 内置的状态码查询对象 #状态码非200视为出错 响应状态码  eg:404 r = requests.get(&amp;#39;http://httpbin.org/status/404&amp;#39;) print(r.status_code)#404 error_info = r.raise_for_status()#Response.raise_for_status()抛出异常 带参数的url请求： #向url传递参数 r = requests.get(url=&amp;#39;http://dict.baidu.com/s&amp;#39;, params={&amp;#39;wd&amp;#39;: &amp;#39;python&amp;#39;})#带参数的GET请求 #当你不知道你的编码类型时 r.encoding = r.apparent_encoding#获取编码类型 print(r.text)#返回解码后的数据 tips 若有图片 r.content 返回bytes数据
eg：r.content r = requests.get(url=&amp;#39;http://music.baidu.com&amp;#39;)#实测，没啥区别 html=r.content #html_doc=str(html,&amp;#39;utf-8&amp;#39;) html_doc=html.decode(&amp;#34;utf-8&amp;#34;,&amp;#34;ignore&amp;#34;) print(html_doc) 响应内容 不同的内容处理方式 Json：request.json() 二进制：一般用于图片 from PIL import Image from io import BytesIO m = request.</description>
    </item>
    
    <item>
      <title>爬虫学习3-网页解析器</title>
      <link>https://euraxluo.gitee.io/blog/posts/crawler/%E7%88%AC%E8%99%AB%E5%88%9D%E7%BA%A73/</link>
      <pubDate>Mon, 22 Oct 2018 00:00:00 +0800</pubDate>
      
      <guid>https://euraxluo.gitee.io/blog/posts/crawler/%E7%88%AC%E8%99%AB%E5%88%9D%E7%BA%A73/</guid>
      <description>BeautifulSoup解析器：    解析器 使用方法 条件     bs4的html解析器 BeautifulSoup(html,&amp;lsquo;html.parser&amp;rsquo;) 安装bs4   lxml的html解析器 BeautifulSoup(html,&amp;lsquo;lxml&amp;rsquo;) pip install lxml   lxml的xml解析器 BeautifulSoup(html,&amp;lsquo;xml&amp;rsquo;) pip install lxml   html5lib的解析器 BeautifulSoup(html,&amp;lsquo;html5lib&amp;rsquo;) pip install html5lib    基本元素    基本元素 说明     tag 标签,&amp;lt;&amp;gt;开头，&amp;lt;/&amp;gt;结尾   name 标签的名字，,&amp;lt;tag&amp;gt;.name   attrs 标签的属性，&amp;lt;tag&amp;gt;.attrs   NavigableString String,&amp;lt;tag&amp;gt;.String   Comment 标签内字符串的注释部分，Comment类型    搜索节点(html中的标签) find_all(name,attrs,recursive,string)</description>
    </item>
    
  </channel>
</rss>
