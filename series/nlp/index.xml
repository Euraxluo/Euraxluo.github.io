<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NLP on Euraxluo Blog</title><link>/series/nlp/</link><description>Recent content in NLP on Euraxluo Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Copyright © 2016-{year} Euraxluo. All Rights Reserved.</copyright><lastBuildDate>Thu, 22 Oct 2020 00:00:00 +0000</lastBuildDate><atom:link href="/series/nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>NLP基础</title><link>/posts/nlp/nlp%E5%9F%BA%E7%A1%80/</link><pubDate>Thu, 22 Oct 2020 00:00:00 +0000</pubDate><guid>/posts/nlp/nlp%E5%9F%BA%E7%A1%80/</guid><description>NLP概览 什么是NLP 自然语言处理,是探讨如何处理及运用自然语言
自然语言认知,是让电脑明白人类的语言
自然语言处理主要包括:文本分析,信息检索,词性标注,问答系统QA
NLP技术 词法分析
- 分词技术 - 词性标注part-of-speech tagging - 命名实体识别NER(识别出3大类和7小类主要用于信息提取,QA,句法分析,机翻元数据标注) 实体边界识别 基于规则和词典进行识别(字典大小,新词?) 基于统计的方法 &amp;gt;隐马尔科夫模型HMM &amp;gt;较大熵ME &amp;gt;支持向量机SVM &amp;gt;条件随机场CRF 确定实体类别(英文,中文需要先分词) - 词义消歧 句法分析
语义分析
常见模型 传统感知机模型
BP神经网络:前馈神经网络(反向传播算法),是现代神经网络的基础
输入层:数据模型的输入,也就是说我们要传入到模型中的数据
隐藏层:用于处理数据,并将处理的结果传递给输出层
输出层:经过隐藏层的计算过后输出的模型内容,分类信息,或者是模型的最终参数
训练过程概述:
正向传播:网络初始化(定义网络参数),隐藏层的输出,输出层的输出
误差计算:通过误差计算的公式,计算出误差
反向传播:通过计算的误差,从输出层向后传播,并在过程中更新权重参数
偏置更新:通过计算的误差,更新隐藏层到输出层,输入层到隐藏层的权重参数
特点:
可以通过逐层信息传递到最后的输出
沿着一条直线计算,直到最后一层,求出计算结果
包含输入层,输出层和隐藏层,目的是实现输入到输出的映射
一般包含多层,并且层与层之间是全链接的不会出现同层和跨层连接
CNN:是一种前馈神经网络,包括卷积层(convolutional)和池化层(pooling layer)
RNN:循环神经网络是一种节点定向连接成环的人工神经网络,这种网络的内部状态可以动态的展示时序行为(短文本)
特点:记忆特性;接受两个参数W和当前时间的特征;参数共享(确保每一步都在做相同的事)
网络结构和BP神经网络的对比:
RNN的类型:
one to one:适合用于分类任务
one to many:文本生成,音乐生成
many to one:多分类任务
many to many(不同维度):翻译任务
many to many(同维度):命名实体识别
LSTM:长短期记忆网络,是一种时间递归神经网络.适合于处理和预测时间序列中间隔和延迟相对较长的重要事件(长文本)
在普通的RNN中增加了一种由门控制的保存单元状态的结构:c
通过遗忘门,输出门,输入门
GRU
只要更新门和重置门,没有隐藏层(可能不太关注时序的各种关系???我不太懂)
双向循环神经网络:
特点:</description></item><item><title>ChatBot模型基础</title><link>/posts/nlp/chatbot%E6%A8%A1%E5%9E%8B/</link><pubDate>Thu, 21 Feb 2019 00:00:00 +0000</pubDate><guid>/posts/nlp/chatbot%E6%A8%A1%E5%9E%8B/</guid><description>word2vec !(http://www.cnblogs.com/neopenx/p/4571996.html)( 是个巨佬) !(https://blog.csdn.net/itplus/article/details/37969817 )
概率语言模型 概率语言模型 预测字符串概率,考虑动机,考虑计算方式
Unigram models(一元文法统计模型)
N-gram 语言模型(N元模型
N元模型 $P( w1,w2,&amp;hellip;,w_m) = i&amp;hellip;m() P(w_i|w1,&amp;hellip;,w_(i-1)) = i&amp;hellip;m() P(w_i|w_(i-n+1),&amp;hellip;,w_(i-1))$
注: n大于3时基本无法处理,参数空间太大.另外它不能表示词与词之间的关联性
神经概率语言模型 在论文《A neural probabilistic language model》中提出的一种模型.该模型的重要工具是词向量
词向量: 对词典D中的任意词w,指定一个固定长度的实值向量$v(w)\in R^m$v(w)就称为w的词向量,m为词向量的长度
概述 训练样本: (Context(w),w) 包括前n-1个词分别的向量,假定每个词向量大小m
投影层： (n-1)*m 首尾拼接起来的大向量
输出: 输出是一棵二叉树,它以语料中出现过的词当做叶子节点.以各词在语料中的出现次数当做权值垢找出来的Huffman树
y_w = (y_w1,y_w2,y_w3,&amp;hellip;y_wN,)
表示上下文为Context(w)时,下一个词恰好为词典中的第i个词的概率
归一化: $$p(w|Context(w)) = \frac{e^{yw,iw}}{\sum^{N}_{i=1}e^{yw,iw} }$$
哈弗曼树 最优二叉树,节点会有权重,指示词的常用频次
使用哈弗曼树
把高频词放在距离根节点近的地方,在测试时,我们每次预测每一层的正概率和负概率
CBOW 根据上下文的词语预测当前词语的出现概率的模型
$$L = \sum_{w\in c}logp(w|Context(w))$$
词向量=哈弗曼编码,经过不断地训练,哈弗曼编码不断改变
权重参数=通过每层的概率计算,最终指向这个词会有一个似然函数,其中的某个参数,就是sigmod函数中的theta
对最终的似然函数求最大==&amp;gt;最大化问题&amp;ndash;&amp;gt;梯度上升
Negative Sampling 为了解决,树空间过大
思想:
保证频次越高的词,越容易被采样出来
不使用哈弗曼树进行预测,使用负采样,降低计算复杂度
Skip-gram seq2seq seq2seq是一个Encoder-Decoder结构的网络,它的输入是一个序列,输出也是一个序列</description></item><item><title>语料处理基础</title><link>/posts/nlp/%E8%AF%AD%E6%96%99%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B/</link><pubDate>Thu, 21 Feb 2019 00:00:00 +0000</pubDate><guid>/posts/nlp/%E8%AF%AD%E6%96%99%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B/</guid><description>语料处理流程 语料收集 &amp;gt; 语料清洗 &amp;gt; 句子向量编码化 &amp;gt; 语料问答对构建 &amp;gt; 语料的模型保存 &amp;gt; 结束 语料收集 聊天记录
电影对话
台词片断
语料清洗 要清洗的内容
多余的空格
不正规的符号
多余的字符,英文
清洗的方法
正则化
切分
好坏语句判断
语料问答对的构建 问答对的处理和拆分 句子向量的编码化 原始文本不能直接训练
将句子转化为向量
将向量转换为句子
语料模型的保存 使用pickle来保存模型
生成pkl格式
利用pkl格式进行语料的训练
最后通过深度模型过后打包成restful
实操 收集语料: 收集了200M的电影台词作为语料
M 你/没/事/吧/？/ M 是/的/，/我/没/事/ M 来/吧/，/我/在/做/早/餐/ M 好/的/ E M C/h/l/o/e/./ M J/a/c/k/!/ M 没/事/吧/?/ M 发/生/什/么/了/?/ M 杀/死/知/道/你/还/活/着/的/人/?/ M 我/不/知/道/,/ M 现/在/,/ /我/要/进/入/C/T/U/的/档/案/ M 秘/密/的 M 我/没/电/脑/不/行/ M 在/加/州/工/学/院/有/个/研/究/处/ M 我/们/能/进/去/ M 那/是/谁/?</description></item></channel></rss>